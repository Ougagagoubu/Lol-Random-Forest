{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading-and-Preprocessing\" data-toc-modified-id=\"Data-Loading-and-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Loading and Preprocessing</a></span></li><li><span><a href=\"#Deep-Learning\" data-toc-modified-id=\"Deep-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Deep Learning</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('2017.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTeamStat(team, df_t):\n",
    "    return df_t.loc[team]\n",
    "\n",
    "def teamsInGame(gameid,df_gid):\n",
    "    return list(df_gid.loc[gameid].index)\n",
    "\n",
    "def getTeamStats1(row,args):\n",
    "    return getTeamStat(row[1],args)\n",
    "\n",
    "def getTeamStats2(row,args):\n",
    "    return getTeamStat(row[2],args)\n",
    "\n",
    "def addTeams1(row,args):\n",
    "    teams = teamsInGame(row[0],args)\n",
    "    return  teams[0]\n",
    "\n",
    "def addTeams2(row,args):\n",
    "    teams = teamsInGame(row[0],args)\n",
    "    return  teams[1]\n",
    "\n",
    "def addResult(row, df_gid):\n",
    "    return df_gid.loc[row[0]][\"result\"][0]/5\n",
    "\n",
    "def buildTeamDF(df):\n",
    "    \n",
    "    #filter KI\n",
    "    df = df[~df.playerid.isin([100,200])]\n",
    "    f_list_sum = [\n",
    "                \"team\",\n",
    "                \"gameid\",    #pkey1\n",
    "                \"side\",      #pkey2\n",
    "                \"result\",\n",
    "                \"k\",\n",
    "                \"d\",\n",
    "                \"a\",\n",
    "                \"fb\",\n",
    "                \"fbvictim\",\n",
    "                \"kpm\",        #müsste team kpm bei rum kommen\n",
    "                \"okpm\",       #opponentteam kpm\n",
    "                \"fd\",\n",
    "                \"ft\",\n",
    "                \"teamdragkills\",\n",
    "                \"dmgtochamps\",\n",
    "                \"dmgtochampsperminute\",\n",
    "                \"wards\",\n",
    "                \"wardkills\",\n",
    "                \"totalgold\",\n",
    "                #\"goldspent\",\n",
    "                \"minionkills\",\n",
    "                \"monsterkills\",\n",
    "                \"monsterkillsownjungle\",\n",
    "                \"monsterkillsenemyjungle\",\n",
    "                \"cspm\",\n",
    "                #\"csat10\",\n",
    "                #\"oppcsat10\",\n",
    "                #\"csdat10\",\n",
    "                \"goldat10\",\n",
    "                \"oppgoldat10\",\n",
    "                \"gdat10\",\n",
    "                \"goldat15\",\n",
    "                \"oppgoldat15\",\n",
    "                #\"gdat15\",\n",
    "                #\"xpat10\",\n",
    "                \"oppxpat10\",\n",
    "                \"xpdat10\"\n",
    "             ]\n",
    "    \n",
    "    #erstellt eine neue Liste newList aus f_list_sum, in dem die Spalten gameid, side und team entfernt werden\n",
    "    itemsToRemove = [\"gameid\",\"side\",\"team\"]\n",
    "    newList = list(filter(lambda x: x not in itemsToRemove, f_list_sum))\n",
    "    \n",
    "    #Wandelt alle Spalten in der Liste newList in einen numerischen Wert um, da beim Import nicht alles \n",
    "    #alsNumeric eingelesen wird\n",
    "    df[newList] = df[newList].apply(pd.to_numeric,args=('errors=coerce',))\n",
    "    df_m = df[f_list_sum].groupby([\"team\",\"gameid\"]).sum()\n",
    "\n",
    "    #ersetzt in der Spalte \"result\" alle Werte 5 durch den Wert 1 (mittels des Dict di)\n",
    "    di = {0:0, 5:1}\n",
    "    df_m[\"result\"].replace(di, inplace=True)\n",
    "    \n",
    "    #Pivotisiere mittels Mittelwerte\n",
    "    df_t = df_m\n",
    "    df_t = df_t[newList].groupby([\"team\"]).mean()\n",
    "    \n",
    "    #erstelle neuen df df_gid mittels Summe\n",
    "    df_gid = df[f_list_sum].groupby([\"gameid\",\"team\"]).sum()\n",
    "    \n",
    "    #erstelle Liste der Game Id's und setze sie als Index und Spalte eines neuen DF df_final\n",
    "    gameids = list(df_gid.index.levels[0])\n",
    "    df_final = pd.DataFrame(gameids, index=gameids)\n",
    "    \n",
    "    #füge df_final die die jeweils an Match mit gid (beretis in df_final enthalten)\n",
    "    #beteiligten Teams ein\n",
    "    df_final['team1'] = df_final.apply (addTeams1, args=(df_gid,), axis=1)\n",
    "    df_final['team2'] = df_final.apply (addTeams2, args=(df_gid,), axis=1)\n",
    "\n",
    "    #füge für jedes der Teams die (jeweils gemittelten) Stats aus DF df_t ein\n",
    "    l_t1 = [\"t1_\" + c for c in df_t.columns]\n",
    "    l_t2 = [\"t2_\" + c for c in df_t.columns]\n",
    "\n",
    "    df_final[l_t1] = df_final.apply (getTeamStats1, args=(df_t,), axis=1)\n",
    "    df_final[l_t2] = df_final.apply (getTeamStats2, args=(df_t,), axis=1)\n",
    "    \n",
    "    #Füge jetzt noch das Endergebnis des Matches ein. Aussage der Spalte \"result\" ist: Team 1 hat gewonnen\n",
    "    df_final[\"result\"] = df_final.apply (addResult, args=(df_gid,), axis=1)\n",
    "    \n",
    "    return (df_final, df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A83900760\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "df_final, df_t = buildTeamDF(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A83900760\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "df_test, df_t_test = buildTeamDF(pd.read_excel('2018.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm, fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = tf.feature_column.numeric_column(df_final[\"t1_k\"])\n",
    "k2 = tf.feature_column.numeric_column(df_final[\"t2_k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "features = np.array(df_final.fillna(0).drop([\"result\",0,\"team1\",\"team2\"], axis=1)).astype(np.float32)\n",
    "labels = np.array(df_final[\"result\"]).astype(np.int32)\n",
    "#labels = np.transpose([labels, 1-labels])\n",
    "\n",
    "features_test = np.array(df_test.fillna(0).drop([\"result\",0,\"team1\",\"team2\"], axis=1)).astype(np.float32)\n",
    "labels_test = np.array(df_test[\"result\"]).astype(np.int32)\n",
    "\n",
    "#data = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder_test, labels_placeholder))\n",
    "#dataset_test = tf.contrib.data.Dataset.from_tensor_slices(#(features_placeholder_test, labels_placeholder))\n",
    "\n",
    "#iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return tf.where(z < 0, alpha * (tf.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1:  Tensor(\"hidden1/mul_1:0\", shape=(?, 400), dtype=float32)\n",
      "hidden2:  Tensor(\"hidden2/mul_1:0\", shape=(?, 300), dtype=float32)\n",
      "hidden3:  Tensor(\"hidden3/mul_1:0\", shape=(?, 300), dtype=float32)\n",
      "logits:  Tensor(\"outputs/BatchNorm/batchnorm/add_1:0\", shape=(?, 2), dtype=float32)\n",
      "loss:  Tensor(\"loss/loss:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (3090, 56) for Tensor 'x:0', which has shape '(?, 58)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7c372ad9b0a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    973\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    976\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (3090, 56) for Tensor 'x:0', which has shape '(?, 58)'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate= 0.001\n",
    "\n",
    "x = tf.placeholder(dtype = features.dtype, shape = (None, 58), name=\"x\")\n",
    "y = tf.placeholder(dtype = tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(),name=\"is_training\")\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn = batch_norm,\n",
    "        normalizer_params = bn_params):\n",
    "    hidden1 = fully_connected(x, 400, activation_fn=selu, scope=\"hidden1\")\n",
    "    #dropout1 = tf.layers.dropout(inputs=hidden1, rate=0.4, training=is_training)\n",
    "\n",
    "    hidden2 = fully_connected(hidden1, 300, activation_fn=selu, scope=\"hidden2\")\n",
    "    dropout2 = tf.layers.dropout(inputs=hidden2, rate=0.2, training=is_training)\n",
    "    \n",
    "    hidden3 = fully_connected(dropout2, 300, activation_fn=selu, scope=\"hidden3\")\n",
    "    \n",
    "    hidden4 = fully_connected(hidden3, 300, activation_fn=selu, scope=\"hidden4\")\n",
    "\n",
    "    hidden5 = fully_connected(hidden4, 200, activation_fn=selu, scope=\"hidden5\")\n",
    "    #dropout3 = tf.layers.dropout(inputs=hidden3, rate=0.1, training=is_training)\n",
    "    \n",
    "    hidden6 = fully_connected(hidden5, 100, activation_fn=selu, scope=\"hidden6\")\n",
    "    \n",
    "    hidden7 = fully_connected(hidden6, 50, activation_fn=selu, scope=\"hidden7\")\n",
    "\n",
    "    logits = fully_connected(hidden7, 2, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    one_xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    one = tf.constant(1, dtype=tf.float32) - tf.reduce_mean(one_xentropy, name=\"one\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"Loss\", loss)\n",
    "accuracy_summary = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "print(\"hidden1: \", hidden1)\n",
    "print(\"hidden2: \", hidden2)\n",
    "print(\"hidden3: \", hidden3)\n",
    "print(\"logits: \", logits)\n",
    "print(\"loss: \", loss)\n",
    "#print(\"predicted_labels: \", correct_pred)\n",
    "\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "loss_summary = tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"C:/tmp/tflogs\", sess.graph)\n",
    "    init.run()\n",
    "    \n",
    "    for i in range(100000):\n",
    "        sess.run(training_op, feed_dict={is_training: True, x: features, y: labels})\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            \n",
    "            summary_str = loss_summary.eval(feed_dict={is_training: False, x: features, y: labels})\n",
    "            summary_acc = accuracy_summary.eval(feed_dict={is_training: False, x: features, y: labels})\n",
    "            writer.add_summary(summary_str, i)\n",
    "            writer.add_summary(summary_acc, i)\n",
    "            writer.flush()\n",
    "            acc_train = accuracy.eval(feed_dict={is_training: False, x: features, y: labels})\n",
    "            acc_test = accuracy.eval(feed_dict={is_training: False, x: features_test, y: labels_test})\n",
    "            print(i, \"Batch: {0:.1%}\".format(acc_train),\" --- Validation: {0:.1%}\".format(acc_test))\n",
    "\n",
    "    #print(accuracy.eval(feed_dict={x: features[0]}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
